{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling alpha and lambda parameters\n",
    "... from a multivariate normal distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the summary stats from the grid search to fit distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_root = \"Q:\"\n",
    "data_path = r\"\\Shared drives\\Pandemic Data\"\n",
    "model_name = \"slf_model\"\n",
    "run_name = \"slf_grid_broad\" # folder of your grid search \n",
    "total_runs = 80 # Count of runs expected (this is only needed if you had variable #'s of runs - ie. from two rounds of parameter sampling)\n",
    "\n",
    "data_dir = f\"{google_root}{data_path}\\{model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_dir = f\"{data_dir}/outputs/summary_stats/{run_name}\"\n",
    "# input_dir = \"inputs\"\n",
    "input_dir = f\"{data_dir}/inputs/noTWN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.read_csv(f\"{stats_dir}/summary_stats_wPrecisionRecallF1FBetaAggProb.csv\")\n",
    "stats = stats.groupby(\"sample\").filter(lambda x: len(x) == total_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregating needed stats by sample\n",
    "\n",
    "agg_dict = {\n",
    "    \"start\":[\"max\"],\n",
    "    \"alpha\":[\"max\"],\n",
    "    \"lamda\": [\"max\"],\n",
    "    \"count_known_countries_time_window_fbeta\": [\"mean\",\"std\"]\n",
    "}\n",
    "\n",
    "agg_dict = {**agg_dict}\n",
    "\n",
    "agg_df = stats.groupby(\"sample\").agg(agg_dict)\n",
    "\n",
    "agg_df.columns = [\"_\".join(x) for x in agg_df.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = agg_df.rename(columns={\"start_max\":\"start\",\"alpha_max\":\"alpha\",\"lamda_max\":\"lamda\",\"count_known_countries_time_window_fbeta_mean\":\"fbeta\"})\n",
    "agg_df['st_err']=agg_df['count_known_countries_time_window_fbeta_std']/np.sqrt(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring possible quantile thresholds for fbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vals = []\n",
    "min_fbeta = []\n",
    "\n",
    "for val in range(70,100):\n",
    "    subset = agg_df.loc[agg_df['fbeta']>=agg_df['fbeta'].quantile(val/100)]\n",
    "    count_vals.append(len(subset.index))\n",
    "    min_fbeta.append(subset['fbeta'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_stats = pd.DataFrame({\"quantile\":range(70,100), \"count\":count_vals, \"min_fbeta\":min_fbeta}).set_index(\"quantile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_threshold = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many samples and what Fbeta scores are captured with each threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sample_stats[\"count\"].plot(ax = ax1)\n",
    "ax1.vlines(quant_threshold, ymin=sample_stats[\"count\"].min(), ymax=sample_stats[\"count\"].max(), linestyle='dashed', color=\"firebrick\")\n",
    "ax1.set_title(\"Count\")\n",
    "\n",
    "sample_stats[\"min_fbeta\"].plot(ax = ax2)\n",
    "ax2.vlines(quant_threshold, ymin=sample_stats[\"min_fbeta\"].min(), ymax=sample_stats[\"min_fbeta\"].max(), linestyle='dashed', color=\"firebrick\")\n",
    "ax2.set_title(\"Fbeta\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the distributions of alpha and lamda look like with that threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df['top']=np.where(agg_df['fbeta']>=agg_df['fbeta'].quantile(quant_threshold/100),'top','low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The full faceted plot - but it is slow to run (and long)\n",
    "\n",
    "# ax = sns.relplot(x=\"lamda\",y=\"fbeta\", row=\"alpha\",hue=\"top\",palette=\"rocket\",\n",
    "#             col=\"start\",data=agg_df,edgecolor=\"black\",linewidth=0.5,s=100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha by year\n",
    "\n",
    "ax = sns.relplot(x=\"alpha\",y=\"fbeta\", col=\"start\",hue=\"top\",palette=\"rocket\",data=agg_df,edgecolor=\"black\",linewidth=0.5,s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lamda by year\n",
    "\n",
    "ax = sns.relplot(x=\"lamda\",y=\"fbeta\", col=\"start\",hue=\"top\",palette=\"rocket\",data=agg_df,edgecolor=\"black\",linewidth=0.5,s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top parameter distribution plot\n",
    "\n",
    "ax = sns.relplot(x=\"alpha\", y=\"lamda\", col=\"start\", hue=\"fbeta\", palette=\"mako_r\", data=agg_df.loc[agg_df['top']==\"top\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the multivariate normal distribution and sampled parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits a separate distribution per year \n",
    "\n",
    "param_samples_df = pd.DataFrame(columns=['alpha','lamda','start'])\n",
    "n = 500\n",
    "\n",
    "for year in [2005,2006,2007]:\n",
    "    top_sets=agg_df.loc[((agg_df['top']==\"mid\") | (agg_df['top']==\"top\")) & (agg_df['start'] == year)][[\"alpha\",\"lamda\",\"fbeta\"]]\n",
    "    param_mean = np.mean(top_sets[[\"alpha\",\"lamda\"]].values, axis=0)\n",
    "    param_cov = np.cov(top_sets[[\"alpha\",\"lamda\"]].values, rowvar=0)\n",
    "    param_sample = np.random.multivariate_normal(param_mean, param_cov, n)\n",
    "    alpha = param_sample[:,0]\n",
    "    lamda = param_sample[:,1]\n",
    "    start = [year]*n\n",
    "    param_sample_df = pd.DataFrame({\"alpha\":alpha, \"lamda\":lamda, \"start\":start})\n",
    "    param_samples_df = pd.concat([param_samples_df, param_sample_df])\n",
    "\n",
    "    print(f\"Year: {year}, Means: {param_mean}, Covariance Matrix: {param_cov}\")\n",
    "    \n",
    "\n",
    "param_samples_df = param_samples_df.loc[param_samples_df['alpha']<=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot to visually examine - should show similar patterns to top parameter distribution plot above\n",
    "\n",
    "ax = sns.relplot(x=\"alpha\", y=\"lamda\", col=\"start\", data=param_samples_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing out 1,000 sampled parameters to runs\n",
    "Use this to replace the content of the commands.txt text file on HPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(agg_df.loc[((agg_df['top']==\"mid\") | (agg_df['top']==\"top\"))])\n",
    "count2005 = len(agg_df.loc[((agg_df['top']==\"mid\") | (agg_df['top']==\"top\")) & (agg_df['start'] == 2005)])\n",
    "count2006 = len(agg_df.loc[((agg_df['top']==\"mid\") | (agg_df['top']==\"top\")) & (agg_df['start'] == 2006)])\n",
    "count2007 = len(agg_df.loc[((agg_df['top']==\"mid\") | (agg_df['top']==\"top\")) & (agg_df['start'] == 2007)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round2005 = round(count2005/total * 1000)\n",
    "round2006 = round(count2006/total * 1000)\n",
    "round2007 = round(count2007/total * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round2005 + round2006 + round2007 == 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples2005 = param_samples_df.loc[param_samples_df['start']==2005].reset_index(drop=True)[0:round2005]\n",
    "samples2006 = param_samples_df.loc[param_samples_df['start']==2006].reset_index(drop=True)[0:round2006]\n",
    "samples2007 = param_samples_df.loc[param_samples_df['start']==2007].reset_index(drop=True)[0:round2007]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples1000 = pd.concat([samples2005,samples2006,samples2007]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 random samples\n",
    "\n",
    "commands = \"\"\n",
    "\n",
    "start_run = 0 \n",
    "end_run = 0\n",
    "\n",
    "script = \"python model_run_args.py\"\n",
    "\n",
    "for i in samples1000.index:\n",
    "    commands += (\n",
    "        \" \".join(\n",
    "            [\n",
    "                script,\n",
    "                str(round(samples1000['alpha'][i],4)),\n",
    "                str(round(samples1000['lamda'][i],4)),\n",
    "                str(samples1000['start'][i]),\n",
    "                str(start_run),\n",
    "                str(end_run),\n",
    "            ]\n",
    "        )\n",
    "        + \"\\n\"\n",
    "    )\n",
    "print(commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the aggregated stats of the sampled params\n",
    "After you run the model with the sampled parameters, to generate the overall summary statistics (across samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"slf_sampled_params\" # Set this to the name of the runs you ran with the sampled parameters\n",
    "stats_dir = f\"{data_dir}/outputs/summary_stats/{run_name}\"\n",
    "# input_dir = \"inputs\"\n",
    "input_dir = f\"{data_dir}/inputs/noTWN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.read_csv(f\"{stats_dir}/summary_stats_wPrecisionRecallF1FBetaAggProb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from summary_stats.py to define variables\n",
    "\n",
    "sim_years = [2014,2020]\n",
    "coi = \"USA\"\n",
    "\n",
    "year_probs_dict_keys = []\n",
    "for year in sim_years:\n",
    "    year_probs_dict_keys.append(f\"prob_by_{year}_{coi}\")\n",
    "\n",
    "validation_df = pd.read_csv(\n",
    "        input_dir + \"/first_records_validation.csv\", header=0, index_col=0,\n",
    "    )\n",
    "\n",
    "countries_dict_keys = []\n",
    "for ISO3 in validation_df.index:\n",
    "    countries_dict_keys.append(f\"diff_obs_pred_metric_{ISO3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from summmary_stats.py to define functions\n",
    "\n",
    "def mse(x):\n",
    "    return sum(x) / len(x)\n",
    "\n",
    "\n",
    "def avg_std(x):\n",
    "    \"\"\"\n",
    "    Compute average standard deviation when aggregating across runs\n",
    "    of a parameter sample\n",
    "    \"\"\"\n",
    "    return math.sqrt(sum(x ** 2) / len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = stats.copy()\n",
    "summary_stat_path = stats_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from get_stats.py to aggregate summary stats\n",
    "\n",
    "agg_dict = {\n",
    "    \"start\": [\"max\"],\n",
    "    \"alpha\": [\"max\"],\n",
    "    \"lamda\": [\"max\"],\n",
    "    \"total_countries_intros_predicted\": [\"mean\", \"std\"],\n",
    "    \"diff_total_countries\": [\"mean\", \"std\"],\n",
    "    \"diff_total_countries_sqrd\": [mse],\n",
    "    \"count_known_countries_time_window\": [\"mean\", \"std\"],\n",
    "    \"diff_obs_pred_metric_mean\": [\"mean\"],\n",
    "    \"diff_obs_pred_metric_stdev\": [avg_std],\n",
    "    \"count_known_countries_time_window_recall\": [\"mean\"],\n",
    "    \"count_known_countries_time_window_precision\": [\"mean\"],\n",
    "    \"count_known_countries_time_window_f1\": [\"mean\"],\n",
    "    \"count_known_countries_time_window_fbeta\": [\"mean\"],\n",
    "}\n",
    "prob_agg_dict = dict(\n",
    "    zip(year_probs_dict_keys, [\"mean\" for i in range(len(year_probs_dict_keys))])\n",
    ")\n",
    "countries_agg_dict = dict(\n",
    "    zip(\n",
    "        countries_dict_keys,\n",
    "        [[\"mean\", \"std\"] for i in range(len(countries_dict_keys))],\n",
    "    )\n",
    ")\n",
    "\n",
    "agg_dict = {**agg_dict, **prob_agg_dict, **countries_agg_dict}\n",
    "\n",
    "agg_df = data.groupby('run_num').agg(agg_dict)\n",
    "\n",
    "agg_df.columns = [\"_\".join(x) for x in agg_df.columns.values]\n",
    "agg_df.to_csv(summary_stat_path + \"/summary_stats_overall.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "948a6e006881c847639198d4e28507cd0955feff6e008072919ba7456f12f8bf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('Pandemic': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
