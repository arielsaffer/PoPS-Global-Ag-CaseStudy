{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Acquisition and Formatting\n",
    "\n",
    "This notebook provides the workflow for aquiring and formatting the data needed to run the Pandemic Network Model. To run this notebook, the following are assumed:\n",
    "- Data are saved in a project folder (e.g., Google Drive root H:/Project Folder/)\n",
    "- Cloned the Pandemic GitHub repository (git clone https://github.com/ncsu-landscape-dynamics/Pandemic_Model.git)\n",
    "- Notebook was launched from the notebook folder of the cloned repo\n",
    "- Already have the following data available or downloaded:\n",
    "    - Koppen-Geiger Climate Classification raster (e.g., Beck_KG_V1_present_0p083.tif from http://koeppen-geiger.vu-wien.ac.at/data)\n",
    "    - Phytosanitary capacity (data frame with country name and ISO3 code, estimate/index of phytosanitary capacity)\n",
    "    - Binary host map raster\n",
    "    - File with the following environmental variables:\n",
    "        - DATA_PATH (file path to data folder that will contain original data, formatted model input data, and model output data)\n",
    "        - COMTRADE_AUTH_KEY (API key to query and download data from the UN Comtrade Database)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys\r\n",
    "import os\r\n",
    "import subprocess\r\n",
    "import math\r\n",
    "import glob\r\n",
    "import json\r\n",
    "import requests\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import geopandas as gpd\r\n",
    "import rasterio\r\n",
    "from rasterio.enums import Resampling\r\n",
    "from rasterstats import zonal_stats\r\n",
    "import dotenv \r\n",
    "from scipy.spatial import distance\r\n",
    "from functools import reduce"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# If notebook was launched from notebook folder of the clone GitHub\r\n",
    "# repository, then set working directory to level above \r\n",
    "# (e.g., /Pandemic_Model)\r\n",
    "os.chdir('../')\r\n",
    "print(os.getcwd())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from Data.Comtrade.get_comtrade import query_comtrade\r\n",
    "from pandemic.generate_trade_forecasts import simple_trade_forecast\r\n",
    "from pandemic.helpers import distance_between\r\n",
    "from pandemic.ecological_calculations import create_climate_similarities_matrix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set Environmental Variables and Paths"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "repo_path = os.getcwd() # should be one folder above launch directory\r\n",
    "print(repo_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Provide file paths to where .env file is saved and load file\r\n",
    "project_path = str(input())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env_file = os.path.join(project_path, '.env') \n",
    "dotenv.load_dotenv(env_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Root project data folder\r\n",
    "data_path = os.getenv('DATA_PATH')\r\n",
    "\r\n",
    "# Path to formatted model inputs\r\n",
    "input_dir = data_path + \"inputs/noTWN\"\r\n",
    "\r\n",
    "# Path to save outputs\r\n",
    "out_dir = data_path + \"outputs/\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Confirm all directories exist:\n",
    "dir_list = [data_path, input_dir, out_dir]\n",
    "for d in dir_list:\n",
    "    if not os.path.exists(d):\n",
    "        print(f\"ERROR:\\t{d} does not exist, creating directory...\")\n",
    "        os.makedirs(d)\n",
    "    else:\n",
    "        print(f\"Confirmed:\\t{d}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Add model data intput and output paths to environment file for \n",
    "# use in run_model.ipynb\n",
    "print(dotenv.set_key(env_file, \"INPUT_PATH\", input_dir))\n",
    "print(dotenv.set_key(env_file, \"OUTPUT_PATH\", out_dir))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Country Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "add_TWN = False\n",
    "if add_TWN:\n",
    "    suffix = \"_wTWN\"\n",
    "else:\n",
    "    suffix = \"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "countries_geo_path = \"H:/Shared drives/APHIS  Projects/Pandemic/Data/Country_list_shapefile/TM_WORLD_BORDERS-0.3/TM_WORLD_BORDERS-0.3.shp\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "countries_gdf = gpd.read_file(countries_geo_path)\n",
    "countries_gdf.iloc[136,4] = 'Macao'\n",
    "countries_gdf.iloc[169,4] = 'Réunion'\n",
    "countries_gdf.iloc[17,4] = 'Myanmar'\n",
    "countries_gdf.iloc[245, 4] = 'Saint Barthelemy'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Koppen-Geiger Climate Classification by Country\n",
    "This step only needs to be run once. \n",
    "\n",
    "It creates a data frame consisting of countries as rows, climate\n",
    "classification codes as columns, and percent area in each country\n",
    "as values. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Read Koppen raster\n",
    "koppen_rast = rasterio.open(\"H:/Shared drives/Data/Raster/Global/Beck_KoppenClimate/Beck_KG_V1_present_0p083.tif\")\n",
    "koppen_arr = koppen_rast.read(1)\n",
    "koppen_arr.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Read Human Influence Index layer (already resampled to match Koppen raster)\r\n",
    "hii = rasterio.open(\"H:/Shared drives/APHIS  Projects/Pandemic/Data/land_use/human_influence_index/hii_v2_resamp.tif\")\r\n",
    "hii_arr = hii.read(1)\r\n",
    "hii_arr = hii_arr.astype('float64')\r\n",
    "hii_arr.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create mask to exclude areas with values below the threshold \r\n",
    "# from the % area calculations of climate similaritiy and \r\n",
    "# host availability \r\n",
    "threshold_val = 16\r\n",
    "\r\n",
    "# Mask values less than threshold (water is already 255)\r\n",
    "hii_arr[hii_arr < threshold_val] = 0\r\n",
    "hii_arr[hii_arr == 255] = 0\r\n",
    "\r\n",
    "# Keep areas greater than threshold\r\n",
    "hii_arr[hii_arr >= threshold_val] = 1\r\n",
    "hii_arr[hii_arr == 0] = np.nan"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Mask selected areas\r\n",
    "koppen_masked = hii_arr * koppen_arr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Calculate zonal statistics\r\n",
    "affine = koppen_rast.transform\r\n",
    "stats = zonal_stats(countries_gdf, koppen_masked, categorical=True, affine = affine)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Add % of each climate classification to countries geodataframe\r\n",
    "koppen_df = countries_gdf.loc[:, ['ISO3', 'NAME']]\r\n",
    "koppen_df['koppen_stats'] = stats\r\n",
    "koppen_df = pd.concat([koppen_df, koppen_df['koppen_stats'].apply(pd.Series)], axis=1).fillna(0)\r\n",
    "pix_ct = pd.DataFrame(koppen_df.sum(axis=1))\r\n",
    "cat_pct = koppen_df.iloc[:,3:].div(pix_ct[0], axis=0, fill_value=None)\r\n",
    "\r\n",
    "kg_codes = pd.read_csv(\"H:/Shared drives/Data/Raster/Global/Beck_KoppenClimate/KGcodes.csv\")\r\n",
    "cat_pct = cat_pct.iloc[:,1:]\r\n",
    "cat_pct.columns = list(kg_codes[\"let\"])\r\n",
    "koppen_df = pd.concat([koppen_df.iloc[:,0:2], cat_pct], axis=1)\r\n",
    "\r\n",
    "koppen_df = koppen_df.fillna(0)\r\n",
    "koppen_df.drop([\"NAME\"], axis=1, inplace=True)\r\n",
    "koppen_df = koppen_df.set_index(\"ISO3\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "koppen_df.to_csv(f\"{input_dir}/koppen_hiiMask{str(threshold_val)}{suffix}.csv\", sep = ',')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# If loading from file:\r\n",
    "#koppen_df = pd.read_csv(f\"{input_dir}/koppen_hiiMask{str(threshold_val)}{suffix}.csv\", index_col=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "koppen_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## UN Comtrade Data\n",
    "\n",
    "This step only needs to be run once per aggregation (e.g., monthly, annual, start year,\n",
    "commodity code). \n",
    "\n",
    "It queries the UN Comtrade API to download data based on the first year\n",
    "of interest, end year (inclusive), commodity codes, frequency (e.g., monthly, annual), \n",
    "and unit value (e.g., value in dollars or net weight). Data are saved as csvs by HS code\n",
    "and time step. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "comtrade_auth_key = os.getenv(\"COMTRADE_AUTH_KEY\")\r\n",
    "start_year = 2000\r\n",
    "end_year = 2019\r\n",
    "temporal_res = 'M'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# To run over separate non-consecutive commodities\r\n",
    "\r\n",
    "commodity_list = [100510,100590]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for commodity in commodity_list:\r\n",
    "    start_commodity = commodity\r\n",
    "    end_commodity = commodity\r\n",
    "\r\n",
    "    query_comtrade(\r\n",
    "        # model_inputs_dir=input_dir,\r\n",
    "        model_inputs_dir=f\"{input_dir}/comtrade{suffix}\",\r\n",
    "        auth_code=comtrade_auth_key,\r\n",
    "        start_code=start_commodity,\r\n",
    "        end_code=end_commodity,\r\n",
    "        start_year=start_year,\r\n",
    "        end_year=end_year,\r\n",
    "        temporal_res=temporal_res,\r\n",
    "        crosswalk_path=f\"H:/Shared drives/APHIS  Projects/Pandemic/Data/un_to_iso{suffix}.csv\")"
   ],
   "outputs": [],
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "start_commodity = commodity_list[0]\r\n",
    "end_commodity = commodity_list[-1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adjust Trade Data for Inflation\n",
    "\n",
    "Get the Consumer Price Index from the US Bureau of Labor Statistics\n",
    "\n",
    "Series CUUR0000SA0L1E - All items less food and energy in U.S. city average, all urban consumers, not seasonally adjusted\n",
    "\n",
    "Other CPI series are available. See more information here: https://www.bls.gov/cpi/\n",
    "\n",
    "### TODO: Write workflow for annual data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Download CPI data\r\n",
    "# API allows 10 years per request, divide year requests into lists of 10 or less\r\n",
    "years = list(range(start_year, end_year + 1))\r\n",
    "year_subsets = [years[x:x+10] for x in range(0, len(years), 10)]\r\n",
    "cpi_series = \"CUUR0000SA0L1E\"\r\n",
    "\r\n",
    "cpi_list = []\r\n",
    "for subset in year_subsets:\r\n",
    "    print(f\"Downloading CPI for {str(subset[0])}-{str(subset[-1])}...\")\r\n",
    "    headers = {\"Content-type\": \"application/json\"}\r\n",
    "    data = json.dumps(\r\n",
    "        {\"seriesid\": [cpi_series], \"startyear\": str(subset[0]), \"endyear\": str(subset[-1])}\r\n",
    "    )\r\n",
    "    p = requests.post(\r\n",
    "        \"https://api.bls.gov/publicAPI/v1/timeseries/data/\", data=data, headers=headers\r\n",
    "    )\r\n",
    "    json_data = json.loads(p.text)\r\n",
    "    json_data = json_data[\"Results\"][\"series\"][0]\r\n",
    "\r\n",
    "    for ts in json_data[\"data\"]:\r\n",
    "        year = ts[\"year\"]\r\n",
    "        period = ts[\"period\"]\r\n",
    "        value = ts[\"value\"]\r\n",
    "        cpi_list.append([year, period, value])\r\n",
    "\r\n",
    "cpi_df = pd.DataFrame(cpi_list, columns=['year', 'period', 'cpi'])\r\n",
    "cpi_df[\"period\"] = cpi_df[\"period\"].str.lstrip('M')\r\n",
    "cpi_df[\"ts\"] = cpi_df[\"year\"] + cpi_df[\"period\"]\r\n",
    "cpi_df = cpi_df.set_index(\"ts\")\r\n",
    "print(f\"CPI for {len(cpi_df)} timesteps downloaded.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Choose baseline year to convert current (nominal) $ to\r\n",
    "base_ts = \"201901\"\r\n",
    "cpi_base = cpi_df.loc[base_ts,\"cpi\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "out_dir"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if temporal_res == 'M':\r\n",
    "    file_list = glob.glob(f\"{input_dir}/comtrade{suffix}/monthly/*/*.csv\")\r\n",
    "    print(f\"Converting current $ to {base_ts} $ for {len(file_list)} files...\")\r\n",
    "    for file in file_list:\r\n",
    "        file_name = file.split(\"\\\\\")[-1]\r\n",
    "        ts = file_name[5:11]\r\n",
    "        cpi_ts = cpi_df.loc[ts,\"cpi\"]\r\n",
    "        adjusted_dir = f\"{input_dir}/comtrade{suffix}/monthly_adjusted/{file_name[:4]}\"\r\n",
    "        if not os.path.exists(adjusted_dir):\r\n",
    "            os.makedirs(adjusted_dir)\r\n",
    "        trade = pd.read_csv(file,index_col=0)\r\n",
    "        trade_adjusted = (trade  * (float(cpi_base) / 100)) / (float(cpi_ts) / 100)\r\n",
    "        trade_adjusted.to_csv(adjusted_dir + \"/\" + file_name)\r\n",
    "    print(f\"Adjusted trade values saved at {input_dir}/comtrade{suffix}/monthly_adjusted/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Aggregated Multiple Commodities\n",
    "\n",
    "This step only needs to be run once per download and if running the model based\n",
    "on all commodities of interest (as opposed to by each commodity) is planned. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if str(start_commodity)[:2] == str(end_commodity)[:2]:\r\n",
    "    code_pre = str(start_commodity)[:2]\r\n",
    "else:\r\n",
    "    code_pre = '-agg'\r\n",
    "print(code_pre)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# If trade data are monthly\r\n",
    "if temporal_res == 'M':\r\n",
    "    file_list = glob.glob(f'{input_dir}/comtrade{suffix}/monthly_adjusted/{start_commodity}/{str(start_commodity)}*.csv')\r\n",
    "    date_list = []\r\n",
    "    for fn in file_list:\r\n",
    "        date = os.path.splitext(fn)[0].split('_')[-1]\r\n",
    "        date_list.append(date)\r\n",
    "\r\n",
    "    date_list_unique = list(set(date_list))\r\n",
    "    date_list_unique.sort()\r\n",
    "    \r\n",
    "    out_path = f'{input_dir}/comtrade{suffix}/monthly_agg/{str(start_commodity)}-{str(end_commodity)}/'\r\n",
    "    if not os.path.exists(out_path):\r\n",
    "        os.makedirs(out_path)\r\n",
    "\r\n",
    "    for d in date_list_unique:\r\n",
    "        d_file_list = glob.glob(input_dir + f'/comtrade{suffix}/monthly_adjusted/*/*{d}*.csv')\r\n",
    "        print(f'{d}: {len(d_file_list)}')\r\n",
    "        dfs = [pd.read_csv(f, sep = \",\", header= 0, index_col=0, encoding='latin1') for f in d_file_list]\r\n",
    "        all_com = reduce(pd.DataFrame.add, dfs)\r\n",
    "        all_com.to_csv(out_path + f\"HS{code_pre}_trades_{d}.csv\")\r\n",
    "        \r\n",
    "# # If trade data are annual\r\n",
    "# if temporal_res == 'A':\r\n",
    "#     out_path = input_dir + f'/comtrade_{suffix}/annual_agg/{str(start_commodity)}-{str(end_commodity)}/'\r\n",
    "#     if not os.path.exists(out_path):\r\n",
    "#         os.makedirs(out_path)\r\n",
    "\r\n",
    "#     year_range = list(range(start_year, end_year + 1, 1))\r\n",
    "#     for d in year_range:\r\n",
    "#         d_file_list = glob.glob(input_dir + f'/comtrade{suffix}/annual/*/*{d}.csv')\r\n",
    "#         print(f'{d}: {len(d_file_list)}')\r\n",
    "#         dfs = [pd.read_csv(f, sep = \",\", header= 0, index_col=0, encoding='latin1') for f in d_file_list]\r\n",
    "#         all_com = reduce(pd.DataFrame.add, dfs)\r\n",
    "#         all_com.to_csv(out_path + f\"HS{code_pre}_trades_{d}.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # Creating annual trade data from the monthly trade data\r\n",
    "# if temporal_res == 'M':\r\n",
    "#     out_path = input_dir + f'/comtrade{suffix}/annual_agg/{str(start_commodity)}-{str(end_commodity)}/'\r\n",
    "#     if not os.path.exists(out_path):\r\n",
    "#         os.makedirs(out_path)\r\n",
    "\r\n",
    "#     year_range = list(range(start_year, end_year + 1, 1))\r\n",
    "#     for d in year_range:\r\n",
    "#         d_file_list = glob.glob(input_dir + f'/comtrade{suffix}/monthly_adjusted/*/*_{d}*.csv')\r\n",
    "#         print(f'{d}: {len(d_file_list)}')\r\n",
    "#         dfs = [pd.read_csv(f, sep = \",\", header= 0, index_col=0, encoding='latin1') for f in d_file_list]\r\n",
    "#         all_com = reduce(pd.DataFrame.add, dfs)\r\n",
    "#         all_com.to_csv(out_path + f\"HS{code_pre}_trades_{d}.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Trade Forecast\n",
    "\n",
    "This step only needs to be run once per aggregation. It is a simple sampling of\n",
    "historical trade data to be used as predictions of future trade values.\n",
    "\n",
    "TO DO: Add ability to include a percent change (e.g., 1% increase) by year or time\n",
    "horizon. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hist_trade_dir = f\"{input_dir}/comtrade{suffix}/monthly_agg/{str(start_commodity)}-{str(end_commodity)}\"\r\n",
    "forecast_dir = f\"{input_dir}/comtrade{suffix}/trade_forecast/monthly_agg/{str(start_commodity)}-{str(end_commodity)}\"\r\n",
    "start_forecast_year = 202001\r\n",
    "number_historical_years = 5\r\n",
    "number_forecast_years = 10\r\n",
    "random_seed = 47"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f\"{input_dir}/comtrade{suffix}/trade_forecast/monthly_agg/{str(start_commodity)}-{str(end_commodity)}\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "simple_trade_forecast(\r\n",
    "    data_dir=f\"{input_dir}/comtrade{suffix}\",\r\n",
    "    output_dir=forecast_dir,\r\n",
    "    start_forecast_date=start_forecast_year,\r\n",
    "    num_yrs_historical=number_historical_years,\r\n",
    "    num_yrs_forecast=number_forecast_years,\r\n",
    "    hist_data_dir=hist_trade_dir,\r\n",
    "    random_seed =random_seed\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # Creating annual trade forecast data from the monthly forecast\r\n",
    "# if temporal_res == 'M':\r\n",
    "#     out_path = input_dir + f'/comtrade{suffix}/trade_forecast/annual_agg/{str(start_commodity)}-{str(end_commodity)}/'\r\n",
    "#     if not os.path.exists(out_path):\r\n",
    "#         os.makedirs(out_path)\r\n",
    "#     forecast_year = int(str(start_forecast_year)[:4])\r\n",
    "#     year_range = list(range(forecast_year, forecast_year + number_forecast_years, 1))\r\n",
    "#     for d in year_range:\r\n",
    "#         d_file_list = glob.glob(input_dir + f'/comtrade{suffix}/trade_forecast/monthly_agg/{str(start_commodity)}-{str(end_commodity)}/*_{d}*.csv')\r\n",
    "#         print(f'{d}: {len(d_file_list)}')\r\n",
    "#         dfs = [pd.read_csv(f, sep = \",\", header= 0, index_col=0, encoding='latin1') for f in d_file_list]\r\n",
    "#         all_com = reduce(pd.DataFrame.add, dfs)\r\n",
    "#         all_com.to_csv(out_path + f\"HS{code_pre}_trades_{d}.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Phytosanitary Capacity Data\n",
    "Incorporates an estimate of phytosanitary capacity for each country. Currently the model \n",
    "is using the proactive value from:\n",
    "\n",
    "Early, R., Bradley, B., Dukes, J. et al. Global threats from invasive alien species in the twenty-first century and national response capacities. Nat Commun 7, 12485 (2016). https://doi-org.prox.lib.ncsu.edu/10.1038/ncomms12485"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "phyto_path = \"H:/Shared drives/APHIS  Projects/Pandemic/Data/phytosanitary_capacity/phytosanitary_capacity_iso3.csv\"\r\n",
    "phyto_df = pd.read_csv(phyto_path, index_col=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "phyto_df= phyto_df[[\"proactive\", \"ISO3\", \"UN\"]]\r\n",
    "phyto_df = phyto_df.rename(columns={\"proactive\": \"Phytosanitary Capacity\"})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "phyto_df.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## NEED TO UPDATE THIS IN add_iso3_phyto.py \r\n",
    "phyto_df = phyto_df.append(\r\n",
    "    {\r\n",
    "        'ISO3': \"USA\", \r\n",
    "        'Phytosanitary Capacity': 3.0, \r\n",
    "        'UN': 840\r\n",
    "    }, ignore_index=True)\r\n",
    "phyto_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Host\n",
    "The step only needs to be run once. \n",
    "\n",
    "Using a binary host map, calculate the percent area in each country with\n",
    "probable presence of host."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Read bindary host raster\r\n",
    "host = rasterio.open('H:/Shared drives/Data/Raster/Global/toh_global_tr.tif')\r\n",
    "host_arr = host.read(1)\r\n",
    "\r\n",
    "# Set large negative values to 0\r\n",
    "host_arr[host_arr < 0.0001] = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "host_arr = np.append(host_arr, np.zeros([hii_arr.shape[0] - host_arr.shape[0], host_arr.shape[1]]), axis=0)\r\n",
    "host_masked = host_arr * hii_arr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Calculate zonal statistics\r\n",
    "affine = host.transform\r\n",
    "stats = zonal_stats(countries_gdf, host_masked, categorical=True, affine = affine)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create host dataframe with country identifiers, results from zonal stats, and calculate host percent area\r\n",
    "host_df = countries_gdf.loc[:, ['ISO3', 'NAME']]\r\n",
    "host_df['host_stats'] = stats\r\n",
    "host_df = pd.concat([host_df, host_df['host_stats'].apply(pd.Series)], axis=1).fillna(0)\r\n",
    "host_df['Host Percent Area'] = (host_df[1.0] / (host_df[0.0] + host_df[1.0])).fillna(0)\r\n",
    "host_df.iloc[136,1] = 'Macao'\r\n",
    "host_df.iloc[169,1] = 'Réunion'\r\n",
    "host_df.iloc[17,1] = 'Myanmar'\r\n",
    "host_df.iloc[245, 1] = 'Saint Barthelemy'\r\n",
    "host_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "host_df.to_csv(f\"{input_dir}/host_hiiMask{str(threshold_val)}{suffix}.csv\", sep = ',')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# If loading from file:\r\n",
    "#host_df = pd.read_csv(f\"{input_dir}/host_hiiMask{str(threshold_val)}{suffix}.csv\", sep=',')\r\n",
    "#host_df.drop(['Unnamed: 0', 'host_stats', '0.0', '1.0'], axis=1, inplace=True)\r\n",
    "#host_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create final countries dataframe\n",
    "Merge country attributes with host percent area, climate classificaiton percent area, \n",
    "and phytosanitary capacity estimates. Filter and order dataframe to match countries \n",
    "with trade data available. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "koppen_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Merge koppen, host, and phytosanitary attributes with countries geodataframe\r\n",
    "countries_gdf = countries_gdf.merge(koppen_df, on='ISO3')\r\n",
    "countries_gdf = countries_gdf.merge(host_df[['ISO3', 'Host Percent Area']], how='left', on='ISO3')\r\n",
    "countries_gdf = countries_gdf.merge(phyto_df, how=\"left\", on=\"ISO3\", suffixes=[None, \"_y\"])\r\n",
    "countries_gdf.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get unique values of phytosanitary capacity for rescaling \r\n",
    "unique_keys = list(countries_gdf['Phytosanitary Capacity'].unique())\r\n",
    "unique_keys.sort()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Rescale input phytosanitary capacity values using specified\r\n",
    "# minimum and maxmimum values\r\n",
    "\r\n",
    "# Minimum phytosanitary capacity value when rescaled\r\n",
    "scaled_min = 0.3\r\n",
    "\r\n",
    "# Maximum phytosanitary capacity value when rescaled\r\n",
    "scaled_max = 0.8\r\n",
    "\r\n",
    "phyto_dict = {}\r\n",
    "\r\n",
    "for i in unique_keys:\r\n",
    "    if np.isnan(i):\r\n",
    "        phyto_dict[i] = 0\r\n",
    "    else:\r\n",
    "        if np.isnan(unique_keys).any():\r\n",
    "            increments = len(unique_keys) - 1\r\n",
    "        else:\r\n",
    "            increments = len(unique_keys)\r\n",
    "        \r\n",
    "        scale_diff = scaled_max - scaled_min\r\n",
    "        phyto_dict[i] = round((scale_diff / increments) * unique_keys.index(i), 2) + scaled_min\r\n",
    "\r\n",
    "phyto_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "countries_gdf[\"Phytosanitary Capacity\"] = countries_gdf[\"Phytosanitary Capacity\"].replace(phyto_dict) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "countries_gdf.set_index('ISO3', inplace=True)\r\n",
    "countries_gdf.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Read example trade matrix to identify which countries are in the geodataframe but not\r\n",
    "# in the trade data\r\n",
    "example_trade = pd.read_csv(f\"{input_dir}/comtrade{suffix}/monthly_agg/{str(start_commodity)}-{str(end_commodity)}/HS{code_pre}_trades_201001.csv\", header=0, index_col=0, encoding='latin-1')\r\n",
    "country_set = set(countries_gdf.index.values)\r\n",
    "trade_set = set(example_trade.index.values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('# ISO3 codes in countries geopackage:\\t', len(country_set))\r\n",
    "print('# ISO3 code matches:\\t\\t\\t', len(trade_set.intersection(country_set)))\r\n",
    "\r\n",
    "print('Which countries are in the TRADE data but NOT the COUNTRIES geopackage')\r\n",
    "miss_country = trade_set - country_set\r\n",
    "print('\\n', miss_country)\r\n",
    "\r\n",
    "print('Which countries are in the COUNTRIES geopackage but NOT the TRADE data:')\r\n",
    "miss_trade = country_set - trade_set \r\n",
    "print('\\n', miss_trade)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "IMN: Isle of Man\n",
    "PRI: Puerto Rico\n",
    "ALA: Aland Islands\n",
    "LIE: Liechtenstein\n",
    "MTQ: Martinique\n",
    "GUF: French Guiana\n",
    "MAF: Saint-Martin (French part)\n",
    "TWN: Taiwan\n",
    "JEY: Jersey\n",
    "MCO: Monaco\n",
    "GGY: Guernsey\n",
    "GLP: Guadeloupe\n",
    "REU: Réunion\n",
    "VIR: US Virgin Islands\n",
    "BVT: Bouvet Island\n",
    "SJM: Svalbard and Jan Mayen Islands"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Remove countries from the geodataframe that do not have trade data\r\n",
    "countries_filtered = countries_gdf.drop(miss_trade, axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Reorder the countries geodataframe rows to match the \r\n",
    "# trade index order\r\n",
    "index_list = list(example_trade.index.values)\r\n",
    "countries_filtered_reindex = countries_filtered.loc[index_list, :]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "countries_filtered_reindex.reset_index(inplace=True)\r\n",
    "countries_filtered_reindex.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save filtered and reindexed countries data with climate, host %s, and phyto values\r\n",
    "countries_path = f\"{input_dir}/countries_slf_hiiMask{str(threshold_val)}{suffix}.gpkg\"\r\n",
    "countries_filtered_reindex.to_file(countries_path, driver='GPKG')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save full country data frame path to .env file\r\n",
    "print(dotenv.set_key(env_file, \"COUNTRIES_PATH\", countries_path))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Distance Matrix\n",
    "Calculate the distance between each origin-destination country pair. Save \n",
    "as matrix array. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "distances = distance_between(countries_filtered_reindex)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.save(f\"{input_dir}/distance_matrix{suffix}.npy\", distances)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Climate Simiarities Matrix\n",
    "Calculate the similarity between each origin-destination country pair. Save as matrix array."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create an n x n array of climate similarity calculations\r\n",
    "climate_similarities = create_climate_similarities_matrix(\r\n",
    "    array_template=example_trade, countries=countries_filtered_reindex\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.save(f\"{input_dir}/climate_similarities_hiiMask{str(threshold_val)}{suffix}.npy\", climate_similarities)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('Pandemic': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "interpreter": {
   "hash": "3931ba98294e2df56dd0b76119dcd2a7b5512cc0b484e115c93f5d1958f27801"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}